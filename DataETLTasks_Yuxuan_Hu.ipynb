{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfceb9c-2f21-410c-b61b-1edb5524bce0",
   "metadata": {},
   "source": [
    "# Task 1: Converting NetCDF Files to ZARR Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0767f-2c6f-4399-921c-500f894d8a2b",
   "metadata": {},
   "source": [
    "# Step 1: Data Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea3ba0-4ec6-47e2-a225-8e678b7c5b6a",
   "metadata": {},
   "source": [
    "There are three ways to access the `.aust.ipar.nc` file: \n",
    "1. OPENDAP\n",
    "2. HTTPServer\n",
    "3. WMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de9bcd-ba92-4d2f-8cfe-f1adb375da6e",
   "metadata": {},
   "source": [
    "I accessed the `.aust.ipar.nc` file through HTTPServer. The link of the NetCDF file at the first day of 2023 is (https://thredds.aodn.org.au/thredds/fileServer/IMOS/SRS/OC/gridded/aqua/P1D/2023/01/A.P1D.20230101T053000Z.aust.ipar.nc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad49e8-b3c5-4cb3-af93-0542f1545364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://thredds.aodn.org.au/thredds/fileServer/IMOS/SRS/OC/gridded/aqua/P1D/2023/01/A.P1D.20230101T053000Z.aust.ipar.nc'\n",
    "response = requests.get(url) \n",
    "\n",
    "filename = 'A.P1D.20230101T053000Z.aust.ipar.nc'\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "else:\n",
    "    print('Failed to download dataset file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07d636-902e-4c42-a68b-26d7793c24d3",
   "metadata": {},
   "source": [
    "# Step 2: Data Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9df5b-c60f-410e-a8cf-797ec0266839",
   "metadata": {},
   "source": [
    "I used xarray library to open this NetCDF dataset, and to ingest its information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe560bc-78ae-491a-bccf-b62945dc66dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5b7b4-0678-4d3e-8cb8-714c52094a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "ds = xr.open_dataset(filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1056d-762d-4a6c-b7e0-a4f4afc65344",
   "metadata": {},
   "source": [
    "# Step 3: Chunking Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968debfc-749c-42b4-8157-d9f8a120dd5a",
   "metadata": {},
   "source": [
    "Following this, I noticed that 'time', 'latitude', and 'longitude' are 1D, whereas 'ipar' is 3D with a relatively large size. I decided to divide 'ipar' into chunks by 'latitude' and 'longitude'. I simply divided 'ipar' into chunks follows '(1,70,100)', refering that each chunk contains 100 elements. This can be improved by providing more details, such as the partitioning strategy for latitude and longitude, and the specific range that represents a region in terms of latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1821f9-35bf-48cd-b737-3af123ce42f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks={'time':1,'latitude':70, 'longitude':100}\n",
    "chuncked_ds = ds.chunk(chunks)\n",
    "chuncked_ds.to_zarr('zarr_store', mode='w')\n",
    "\n",
    "for var in chuncked_ds.variables:\n",
    "    data_array = chuncked_ds[var]\n",
    "    if isinstance(data_array, xr.DataArray):\n",
    "        print(f\"Variable '{var}':\")\n",
    "        if data_array.chunks:\n",
    "            print(f\" - Chunks: {data_array.chunks}\")\n",
    "        else:\n",
    "            print(\" - Not chunked\")\n",
    "\n",
    "\n",
    "def convert2zarr(filename):\n",
    "    ds = xr.open_dataset(filename)\n",
    "    chunks={'time':1,'latitude':70, 'longitude':100}\n",
    "    chuncked_ds = ds.chunk(chunks)\n",
    "    chuncked_ds.to_zarr('zarr_store', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823257b-7a53-4419-a91a-9e5d47d661c0",
   "metadata": {},
   "source": [
    "# Step 4: Automation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a213da-82f9-4673-829a-a1a6246bcf18",
   "metadata": {},
   "source": [
    "Lastly, I consider how to automatically execute the Zarr conversion to update daily. I noticed that PID distinguished daily datasets. In the form of PID, the date was displayed as YYYYMMDD. For instance, the PID of 01/01/2023 is 20230101T053000Z, and the PID of 02/01/2023 is 20230102T053000Z. Meanhwile, the url shows a pattern as ''https://thredds.aodn.org.au/thredds/fileServer/IMOS/SRS/OC/gridded/aqua/P1D/**YEAR**/**MONTH**/A.P1D.**PID**.aust.ipar.nc''\n",
    "\n",
    "The dataset of current day can be extracred with the following function `auto_conversion()`.\n",
    "\n",
    "The whole automatic conversion process is demonstrated as:\n",
    " 1. create a python file with the `auto_conversion` code\n",
    " 2. deploy this program on the server\n",
    " 3. set up a schedule to run this program daily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69e348-f183-4c29-8164-a2ea4324b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def auto_conversion():\n",
    "    today = datetime.now()\n",
    "    year = today.year\n",
    "    month = f'{today.month:02d}'\n",
    "    current_date = today.strftime(\"%Y%m%d\")\n",
    "    url = f'https://thredds.aodn.org.au/thredds/fileServer/IMOS/SRS/OC/gridded/aqua/P1D/{year}/{month}/A.P1D.{current_date}T053000Z.aust.ipar.nc'\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = f'A.P1D.{current_date}T053000Z.aust.ipar.nc'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        convert2zarr(filename)\n",
    "        print('Successfully convet to Zarr format')\n",
    "    else:\n",
    "        print('Failed to download dataset file')\n",
    "\n",
    "auto_conversion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f51cc-7a7c-4d2b-8a5d-aeedec5cf11c",
   "metadata": {},
   "source": [
    "Notably, this depends on the update schema of the NetCDF dataset. Specifically, the program should run after the data becomes available on the HTTP Server. Therefore, the aforementioned function can be enhanced to collect a specific dataset with a given date (in the format of 'YYYYMMDD') as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504b6313-1ef3-4c3e-a811-60e380e74cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def auto_conversion_by_date(date):\n",
    "    year = date[:4]\n",
    "    month = date[4:6]\n",
    "    url = f'https://thredds.aodn.org.au/thredds/fileServer/IMOS/SRS/OC/gridded/aqua/P1D/{year}/{month}/A.P1D.{date}T053000Z.aust.ipar.nc'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = f'A.P1D.{date}T053000Z.aust.ipar.nc'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        convert2Zarr(filename)\n",
    "        print('Successfully convet to Zarr format')\n",
    "    else:\n",
    "        print('Failed to download dataset file')\n",
    "\n",
    "auto_conversion_by_date('20240201')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868e1c6-7311-41e5-a424-9779d190116e",
   "metadata": {},
   "source": [
    "# Task 2: Converting CSV to GeoParquet\n",
    "This task requires to convert the `abs-regional-lga-2021` dataset from a csv file to the GeoParquet format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bda2f-5093-4410-9d3c-9853ba8a7cfb",
   "metadata": {},
   "source": [
    "# Step 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737e363-baaa-45c1-ba89-1d176e3a690e",
   "metadata": {},
   "source": [
    "After go through the dataset, I determined these tasks to clean the source data:\n",
    "- Check and remove duplicate values: identify and remove duplicate data.\n",
    "- Process empty values: identify empty values in the dataset and decide how to handle them (e.g., remove rows with empty values or fill in empty values).\n",
    "- Check data types: Ensure that each column of data is of the correct type (as denoted in column 'UNIT_MEASURE: Unit of Measure')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5097289-dfa6-4527-994d-bb8eff2bd2ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f210611-b00e-4d9e-a254-314e640b842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baddd11-648d-4eb5-b74e-a5abb291c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'ABS_ABS_REGIONAL_LGA2021_1.0.0.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "parquet_file = \"s3://gbr-dms-data-public/abs-regional-lga-2021/data.parquet\"\n",
    "parquet_df = pd.read_parquet(parquet_file, storage_options={'anon': True})\n",
    "print(parquet_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603ce17-27ff-4afb-b40f-e4b7ff8e49ff",
   "metadata": {},
   "source": [
    "By analysing the expected formats of results, I preprocessed the dataset to ensure data align with the expected outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9355d-fc39-42c0-a1f6-2642a405b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FREQUENCY'] = df['FREQUENCY: Frequency'].apply(lambda x:x.split(':')[0])\n",
    "df['REGIONTYPE'] = df['REGIONTYPE: Region Type'].apply(lambda x:x.split(':')[0])\n",
    "df['REGION_CODE'] = df['LGA_2021: Region'].apply(lambda x:x.split(':')[0])\n",
    "df['REGION_NAME'] = df['LGA_2021: Region'].apply(lambda x:x.split(':')[1])\n",
    "df['MEASURE'] = df['MEASURE: Data Item'].apply(lambda x:x.split(':')[0])\n",
    "df.loc[df['UNIT_MULT: Unit of Multiplier'].notnull(), 'OBS_VALUE'] = df['OBS_VALUE'] * 1000000\n",
    "\n",
    "df = df.drop(['FREQUENCY: Frequency', 'REGIONTYPE: Region Type', 'LGA_2021: Region', 'MEASURE: Data Item', \n",
    "              'UNIT_MEASURE: Unit of Measure','OBS_STATUS: Observation Status', 'OBS_COMMENT: Observation Comment', \n",
    "              'UNIT_MULT: Unit of Multiplier'], axis=1)\n",
    "\n",
    "df.rename(columns={'TIME_PERIOD: Time Period': 'TIME_PERIOD'}, inplace=True)\n",
    "column_order = ['DATAFLOW', 'FREQUENCY', 'TIME_PERIOD', 'REGIONTYPE', 'REGION_CODE', 'REGION_NAME', 'MEASURE', 'OBS_VALUE']\n",
    "df = df[column_order]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2f60f-7d0d-4dcf-a701-3ce195757d36",
   "metadata": {},
   "source": [
    "From the printed information,  I observed that columns 0 to 7 are of the same size and contain no null data. To clean the data, I will first check for and remove any duplicate data in these columns, and then address any empty data in the last two columns. As for column 8, which is labeled 'UNIT_MULT: Unit of Multiplier', the values in this column are mostly null, but some of them have the same value, '6: Millions'. This indicates the measured value should be multiplied by one million. IF this column is non-null, I can adjust the values in column 6 'OBS_VALUE' accordingly and drop this column as well.\n",
    "\n",
    "Meanwhile, column 7, labeled 'UNIT_MEASURE', indicates the unit of measurement. Particularly, values representing numbers should be integers.\n",
    "\n",
    "Therefore, the data clean strategy is improved as:\n",
    "- Check and remove duplicate values: identify and remove duplicate data.\n",
    "- Process empty values: adjust values in column 6 'OBS_VALUE' if column 8 'UNIT_MULT' is not-null. Remove columns 8-10.\n",
    "- Check data types: Ensure that each column of data is of the correct type (as denoted in column 'UNIT_MEASURE: Unit of Measure'). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0171930e-7836-4084-a64d-135f421c08a2",
   "metadata": {},
   "source": [
    "# Step 2: Pivot Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26bfd5-0681-4ece-8c85-af237fd6b88c",
   "metadata": {},
   "source": [
    "Since this is a regional dataset, I selected columns 'REGION_CODE', 'REGION_NAME', 'TIME_PERIOD' as rows, 'MEASURE: Data Item' as a column, 'OBS_VALUE' as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f1d75-b166-4af7-8a5b-404f17319179",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = df.pivot_table(\n",
    "    index=['REGION_CODE', 'REGION_NAME', 'TIME_PERIOD'], \n",
    "    columns='MEASURE',\n",
    "    values='OBS_VALUE'\n",
    ")\n",
    "pivot_table = pivot_table.reset_index()\n",
    "df_sorted = pivot_table.sort_values(by=['TIME_PERIOD', 'REGION_CODE'])\n",
    "pivot_table_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "print(pivot_table_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2059375-9f92-4037-89ab-9c58af046881",
   "metadata": {},
   "source": [
    "# Step 3: Add Reference Geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabe77d-dbb5-4283-b3a0-5d0d0f11bd35",
   "metadata": {},
   "source": [
    "Add a geometry column for the pivot table. And access AWS S3 to get reference geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f437159-fd58-41dc-87b6-9949e57f01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential_df = pd.read_csv('rootkey.csv')\n",
    "access_key_id = credential_df.iloc[0, 0]\n",
    "secret_access_key = credential_df.iloc[0, 1]\n",
    "\n",
    "geoms_file = \"s3://gbr-dms-data-public/tasks/geoms.parquet\"\n",
    "geoms_df = pd.read_parquet(geoms_file, storage_options={'anon': True})\n",
    "print(geoms_df)\n",
    "\n",
    "result = pd.merge(pivot_table_sorted, geoms_df, left_on='REGION_CODE', right_on='LGA_CODE21')\n",
    "result = result.drop(['minx', 'miny', 'maxx', 'maxy'], axis=1)\n",
    "\n",
    "result = result.sort_values(by=['TIME_PERIOD', 'REGION_CODE'])\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286a0b3-2508-43d3-b84f-f6ff29bb414a",
   "metadata": {},
   "source": [
    "# Step 4: Convert to the GeoParquet Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dddbdb-4ccb-4fd1-9d61-af2b34f51d19",
   "metadata": {},
   "source": [
    "Convert the updated dataframe to the GeoParquet format through Python package GeoPandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4bb1c-9ace-4dfb-9b06-dfb02d0cf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame(result, geometry='geometry')\n",
    "\n",
    "gdf.to_parquet('data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c780a11-a54c-4ffd-9558-096760547e78",
   "metadata": {},
   "source": [
    "# Step 5: Save to AWS S3\n",
    "Save this dataset in the GeoParquet format to AWS S3. *An error occurs when excuting this cell. This error is due to there is no bucket on my S3 server. It can be replaced to the server which has the bucket*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0f5bd-d887-435c-a987-390ba3cd707a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# Initialize a boto3 client with S3 access\n",
    "s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "\n",
    "file_name = 'data.parquet'\n",
    "folder_name = 'abs-regional-lga-2021'\n",
    "bucket_name = 'gbr-dms-data-public'\n",
    "\n",
    "# Upload the file\n",
    "try:\n",
    "    s3_client.upload_file(file_name, bucket_name, 'folder_name/{}'.format(filename))\n",
    "    print(f\"File uploaded to s3://gbr-dms-data-public/{folder_name}/{file_name}\")\n",
    "except NoCredentialsError:\n",
    "    print(\"Credentials not available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
